{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4371464d",
   "metadata": {},
   "source": [
    "# Spatial Autocorrelation Analysis of Protected Historical Sites in Zurich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7cdd8",
   "metadata": {},
   "source": [
    "## **Impressum**\n",
    "\n",
    "**Author:** Joram Schito  \n",
    "**Affiliation:** ETH Zurich  \n",
    "**Year:** 2025  \n",
    "**License:** MIT  \n",
    "\n",
    "This Jupyter Notebook was developed as part of the CAS RIS course at **ETH Zurich**. The content is licensed under the **MIT License**, allowing for reuse, modification, and distribution with proper attribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb9c038",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "Protected historical sites play a crucial role in preserving cultural heritage, and understanding their spatial distribution is essential for effective urban planning and conservation. This notebook explores the spatial distribution of **Denkmalschutzobjekte (protected historical sites)** in Zurich, focusing on their clustering patterns within the city's statistical districts.\n",
    "\n",
    "Spatial autocorrelation methods, such as **Moran’s I** and **Getis-Ord G\\***, are widely used to analyze geographic phenomena. These methods can help determine whether certain areas of the city exhibit statistically significant clusters of historical sites or whether their distribution appears random. This analysis provides valuable insights for **heritage management**, urban planning, and resource allocation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Purpose of This Notebook**\n",
    "This Jupyter Notebook is designed to guide the analysis of the spatial distribution of **protected historical sites** in Zurich using spatial autocorrelation methods. The purpose is to detect and interpret significant spatial patterns of clustering or dispersion.\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "### **Objectives**\n",
    "1. **Aggregate Point Data to Statistical Zones**  \n",
    "   - Count the number of historical sites per **statistical district**.  \n",
    "   - Use spatial join techniques to associate point data with polygonal areas.  \n",
    "\n",
    "2. **Evaluate Global Spatial Autocorrelation**  \n",
    "   - Compute **Global Moran’s I** to detect overall clustering or dispersion in the dataset.  \n",
    "   - Use **Queen contiguity-based spatial weights** to define neighborhood relationships.  \n",
    "\n",
    "3. **Identify Local Clusters (Hotspots and Coldspots)**  \n",
    "   - Apply **Local Getis-Ord G\\*** analysis to identify statistically significant **hotspot (high-high clusters)** and **coldspot (low-low clusters)** areas.  \n",
    "   - Categorize results using Z-scores to distinguish significant clusters from random patterns.  \n",
    "\n",
    "4. **Visualize Results**  \n",
    "   - Generate **statistical plots** to understand the data distribution.  \n",
    "   - Create **choropleth maps** to represent the spatial patterns of protected sites.  \n",
    "   - Compare different spatial weighting methods to evaluate their impact on results.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Expected Outcomes**\n",
    "- Identification of areas where historical sites **significantly cluster** or **disperse**.  \n",
    "- Insight into **spatial patterns** that can inform urban heritage preservation strategies.  \n",
    "- A reproducible workflow for conducting spatial autocorrelation analysis in **QGIS & Python**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a510e13",
   "metadata": {},
   "source": [
    "## **Spatial Autocorrelation Methods: Global Moran’s I & Getis-Ord G\\***\n",
    "\n",
    "### **1. Global Moran’s I**\n",
    "Global Moran’s I is a measure of **global spatial autocorrelation** that evaluates whether a variable exhibits clustering, dispersion, or a random spatial pattern across a study area.\n",
    "\n",
    "#### **Formula:**\n",
    "$$ \n",
    "I = \\frac{n}{\\sum_{i} \\sum_{j} w_{ij}} \\cdot \n",
    "\\frac{\\sum_{i} \\sum_{j} w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_{i} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "#### **Explanation:**\n",
    "- **$I$** = Moran’s I statistic  \n",
    "- **$n$** = Number of spatial units (polygons or points)  \n",
    "- **$x_i$** = Value of the variable of interest at location $i$  \n",
    "- **$\\bar{x}$** = Mean of the variable of interest  \n",
    "- **$w_{ij}$** = Spatial weight between locations $i$ and $j$  \n",
    "\n",
    "#### **Interpretation:**\n",
    "- **$ I > 0 $** → **Positive spatial autocorrelation** (similar values cluster together).  \n",
    "- **$ I < 0 $** → **Negative spatial autocorrelation** (high values are surrounded by low values, and vice versa).  \n",
    "- **$ I \\approx 0 $** → No significant spatial pattern (random distribution).  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Getis-Ord G\\*** (Local Hotspot Analysis)\n",
    "Getis-Ord G\\* is a **local** spatial statistic that identifies areas of **significantly high (hotspots) or low (coldspots) values** by comparing local averages to global averages.\n",
    "\n",
    "#### **Formula:**\n",
    "$$\n",
    "G_i^* = \\frac{\\sum_{j} w_{ij} x_j}{\\sum_{j} x_j}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "Z(G_i^*) = \\frac{G_i^* - \\bar{G}}{s(G)}\n",
    "$$\n",
    "\n",
    "#### **Explanation:**\n",
    "- **$ G_i^* $** measures **how high or low the values around location $ i $ are, compared to the global average**.  \n",
    "- **$ w_{ij} $** = Spatial weight between locations $ i $ and $ j $  \n",
    "- **$ x_j $** = Value at location $ j $  \n",
    "\n",
    "#### **Interpretation (Z-Scores):**\n",
    "- **$ Z(G_i^*) > 1.96 $** → **Hotspot (High values significantly clustered together)**  \n",
    "- **$ Z(G_i^*) < -1.96 $** → **Coldspot (Low values significantly clustered together)**  \n",
    "- **$ -1.96 < Z(G_i^*) < 1.96 $** → No significant clustering  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9e5f2",
   "metadata": {},
   "source": [
    "## Prerequities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867987d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geopandas libpysal esda mapclassify contextily matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdd007",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecb5d5",
   "metadata": {},
   "source": [
    "### Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7602a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import libpysal\n",
    "import esda\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21618ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load polygon layer (spatial units for analysis)\n",
    "gdf_polygons = gpd.read_file(\"Geostatistics_OGD_Zurich_Lab_Files.gpkg\", layer=\"Statistische_Quartiere_ohne_Wald_ohne_Gewaesser\")\n",
    "\n",
    "# Load point layer (historical sites)\n",
    "# gdf_points = gpd.read_file(\"Geostatistics_OGD_Zurich_Lab_Files.gpkg\", layer=\"Brunnen_in_Quartierzonen\")\n",
    "# gdf_points = gpd.read_file(\"Geostatistics_OGD_Zurich_Lab_Files.gpkg\", layer=\"Denkmalschutzobjekte_Test\")\n",
    "gdf_points = gpd.read_file(\"Geostatistics_OGD_Zurich_Lab_Files.gpkg\", layer=\"Denkmalschutzobjekte_in_Quartierzonen\")\n",
    "\n",
    "# Ensure both datasets are in the same CRS\n",
    "if gdf_polygons.crs != gdf_points.crs:\n",
    "    gdf_points = gdf_points.to_crs(gdf_polygons.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc3299",
   "metadata": {},
   "source": [
    "### Step 2: Count Points in Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial join (points within polygons)\n",
    "gdf_joined = gpd.sjoin(gdf_points, gdf_polygons, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Count the number of points per polygon\n",
    "counts = gdf_joined.groupby(\"index_right\").size()\n",
    "\n",
    "# Add the count column back to the polygons\n",
    "gdf_polygons[\"NUMPOINTS\"] = gdf_polygons.index.map(counts).fillna(0)  # Fill empty values with 0\n",
    "\n",
    "# Check the data\n",
    "print(gdf_polygons[[\"NUMPOINTS\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be9372",
   "metadata": {},
   "source": [
    "### Step 3: Create a Spatial Weights Matrix (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method A\n",
    "# Compute polygon centroids\n",
    "gdf_polygons[\"centroid\"] = gdf_polygons.geometry.centroid\n",
    "coords = np.array(list(zip(gdf_polygons.centroid.x, gdf_polygons.centroid.y)))\n",
    "\n",
    "# Build K-nearest neighbors spatial weights matrix (K=15)\n",
    "knn = libpysal.weights.KNN.from_array(coords, k=8)\n",
    "knn.transform = \"r\"  # Row-standardized weights\n",
    "\n",
    "# Method B.1 - Queen pattern\n",
    "# Create adjacency-based spatial weights, standardize them and show the average numbers of neighbors\n",
    "w_queen = libpysal.weights.Queen.from_dataframe(gdf_polygons, use_index=True)\n",
    "w_queen.transform = \"r\"\n",
    "neighbors_count = np.array([len(w_queen.neighbors[i]) for i in w_queen.neighbors])\n",
    "print(\"Average number of neighbors per polygon:\", np.mean(neighbors_count))\n",
    "\n",
    "# Method B.2 - Rook pattern\n",
    "# Create adjacency-based spatial weights, standardize them and show the average numbers of neighbors\n",
    "w_rook = libpysal.weights.Rook.from_dataframe(gdf_polygons, use_index=True)\n",
    "w_rook.transform = \"r\"\n",
    "neighbors_count = np.array([len(w_rook.neighbors[i]) for i in w_rook.neighbors])\n",
    "print(\"Average number of neighbors per polygon:\", np.mean(neighbors_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86472f0-9698-4a1a-8482-bb434df9e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select either KNN or the Queen contiguity as weight mechanism\n",
    "# weight = knn\n",
    "weight = w_queen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb16e1",
   "metadata": {},
   "source": [
    "The analysis above showed that we have more neighbors in average if we use the Queen's contiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddf3e7",
   "metadata": {},
   "source": [
    "### Step 4: Check for issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(gdf_polygons[\"NUMPOINTS\"], bins=20, kde=True)\n",
    "plt.title(\"Distribution of NUMPOINTS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde73ca",
   "metadata": {},
   "source": [
    "If the data is highly right-skewed, normalize it using a log transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516fd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid log(0) issues\n",
    "gdf_polygons[\"NUMPOINTS_log\"] = np.log1p(gdf_polygons[\"NUMPOINTS\"])\n",
    "\n",
    "# Use the log-transformed values in Getis-Ord Gi*\n",
    "y = gdf_polygons[\"NUMPOINTS_log\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the variance\n",
    "print(\"Variance of NUMPOINTS:\", np.var(gdf_polygons[\"NUMPOINTS\"]))\n",
    "print(\"Variance of NUMPOINTS_log:\", np.var(gdf_polygons[\"NUMPOINTS_log\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd21e1",
   "metadata": {},
   "source": [
    "### Step 5: Compute Global Moran’s I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208d154-f618-4264-8453-21e7c18f44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_moran(y, w, permutations=999, seed=42):\n",
    "    \"\"\"\n",
    "    Computes Moran's I and its significance via permutation testing (matches esda).\n",
    "    \n",
    "    y: numpy array of values (e.g., point counts)\n",
    "    w: spatial weights matrix (W)\n",
    "    permutations: number of random simulations for p-value computation\n",
    "    seed: fixed seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        moran_I (float): Moran's I value\n",
    "        p_sim (float): p-value from permutation test (matches esda)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)  # Fixed random seed\n",
    "\n",
    "    y_mean = np.mean(y)\n",
    "    y_diff = y - y_mean  # Deviation from mean\n",
    "\n",
    "    N = len(y)\n",
    "    W = sum(sum(w.weights[i]) for i in range(N))  # Sum of all weights\n",
    "\n",
    "    numerator = 0\n",
    "    denominator = sum(y_diff ** 2)\n",
    "\n",
    "    for i in range(N):\n",
    "        neighbors = w.neighbors.get(i, [])  # Get neighbors safely\n",
    "        for j in neighbors:\n",
    "            numerator += w.weights[i][neighbors.index(j)] * y_diff[i] * y_diff[j]\n",
    "\n",
    "    moran_I = (N / W) * (numerator / denominator)\n",
    "\n",
    "    # ---- Compute Expected Moran's I ----\n",
    "    E_I = -1 / (N - 1)  # Expected Moran's I under normality\n",
    "\n",
    "    # ---- Permutation Test (for p-value) ----\n",
    "    permuted_Is = []\n",
    "    for _ in range(permutations):\n",
    "        y_perm = np.copy(y)  # Copy y to avoid modifying original data\n",
    "        rng.shuffle(y_perm)  # Shuffle with fixed random generator\n",
    "        perm_diff = y_perm - np.mean(y_perm)\n",
    "\n",
    "        perm_num = 0\n",
    "        for i in range(N):\n",
    "            neighbors = w.neighbors.get(i, [])\n",
    "            for j in neighbors:\n",
    "                perm_num += w.weights[i][neighbors.index(j)] * perm_diff[i] * perm_diff[j]\n",
    "\n",
    "        permuted_Is.append((N / W) * (perm_num / denominator))\n",
    "\n",
    "    permuted_Is = np.array(permuted_Is)\n",
    "\n",
    "    # ---- Compute Standardized Z-score for p-value ----\n",
    "    perm_mean = np.mean(permuted_Is)\n",
    "    perm_std = np.std(permuted_Is)\n",
    "\n",
    "    if perm_std > 0:\n",
    "        z_score = (moran_I - perm_mean) / perm_std\n",
    "    else:\n",
    "        z_score = 0  # Avoid division by zero\n",
    "\n",
    "    # Two-tailed p-value (matches `esda`)\n",
    "    p_sim = np.mean(np.abs(permuted_Is - perm_mean) >= np.abs(moran_I - perm_mean))\n",
    "\n",
    "    return moran_I, E_I, z_score, p_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febc097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the variable for analysis (counts per polygon)\n",
    "numpoints_variable_name = \"NUMPOINTS\"\n",
    "#numpoints_variable_name = \"NUMPOINTS_log\"\n",
    "\n",
    "y = gdf_polygons[numpoints_variable_name].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e900fa-5314-4f85-b172-2e4bd8e9fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Moran’s I (based on the own function)\n",
    "moran_I, E_I, z_score, p_value = compute_moran(y, weight, permutations=120, seed=10000)\n",
    "\n",
    "# Print results\n",
    "print(f\"Global Moran’s I: {moran_I}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Significant spatial clustering detected!\")\n",
    "else:\n",
    "    print(\"No significant spatial pattern.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39139e6a-ab08-4622-ba00-5189bf7a6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Moran’s I (method with the esda package)\n",
    "# moran = esda.moran.Moran(y, knn)\n",
    "moran = esda.moran.Moran(y, weight)\n",
    "\n",
    "# Print results\n",
    "print(f\"Global Moran’s I: {moran.I}\")\n",
    "print(f\"p-value: {moran.p_sim}\")\n",
    "\n",
    "if moran.p_sim < 0.05:\n",
    "    print(\"Significant spatial clustering detected!\")\n",
    "else:\n",
    "    print(\"No significant spatial pattern.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae12da1",
   "metadata": {},
   "source": [
    "### Step 6: Compute Getis-Ord Gi*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorization for different significance levels\n",
    "def categorize_gi(z):\n",
    "    if z > 2.58:\n",
    "        return \"Strong Hotspot (p<0.01)\"\n",
    "    elif z > 1.96:\n",
    "        return \"Moderate Hotspot (p<0.05)\"\n",
    "    elif z > 1.65:\n",
    "        return \"Weak Hotspot (p<0.10)\"\n",
    "    elif z < -2.58:\n",
    "        return \"Strong Coldspot (p<0.01)\"\n",
    "    elif z < -1.96:\n",
    "        return \"Moderate Coldspot (p<0.05)\"\n",
    "    elif z < -1.65:\n",
    "        return \"Weak Coldspot (p<0.10)\"\n",
    "    else:\n",
    "        return \"Not Significant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960f864-63b8-4923-a221-712cf76d4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_g_star(y, w):\n",
    "    \"\"\"\n",
    "    Computes the local Getis-Ord G* statistic manually.\n",
    "    \n",
    "    y: numpy array of values (e.g., point counts)\n",
    "    w: spatial weights matrix (W)\n",
    "    \n",
    "    Returns an array of z-scores.\n",
    "    \"\"\"\n",
    "    mean_y = np.mean(y)\n",
    "    std_y = np.std(y) if np.std(y) > 0 else 1  # Avoid division by zero\n",
    "\n",
    "    z_scores = np.zeros(len(y))\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        neighbors = w.neighbors.get(i, [])  # Get neighbors safely\n",
    "\n",
    "        if not neighbors:\n",
    "            z_scores[i] = 0  # No neighbors, set z-score to 0\n",
    "        else:\n",
    "            weights = w.weights[i]  # Extract weight list\n",
    "\n",
    "            # Ensure weights are a list (not dictionary)\n",
    "            if isinstance(weights, dict):\n",
    "                weights = list(weights.values())  \n",
    "            \n",
    "            w_sum = sum(weights)  # Correctly sum weights\n",
    "            sum_neighbors = sum(y[j] * weights[k] for k, j in enumerate(neighbors))\n",
    "\n",
    "            g_star = (sum_neighbors - mean_y * w_sum) / (std_y * np.sqrt(w_sum))\n",
    "            z_scores[i] = g_star\n",
    "\n",
    "    return z_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Getis-Ord Gi*\n",
    "gdf_polygons[\"Gi*\"] = compute_g_star(y, weight)\n",
    "gdf_polygons[\"Gi*_Category\"] = gdf_polygons[\"Gi*\"].apply(categorize_gi)\n",
    "\n",
    "# Remove the centroid geometry column before saving\n",
    "if \"centroid\" in gdf_polygons.columns:\n",
    "    gdf_polygons = gdf_polygons.drop(columns=[\"centroid\"])\n",
    "\n",
    "# Save results (optional, if you are working locally)\n",
    "# gdf_polygons.to_file(\"Geostatistics_OGD_Zurich_Lab_Files.gpkg\", layer=\"Quartiere_with_GiStar\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# DEBUG\n",
    "print(gdf_polygons['Gi*'])\n",
    "print('')\n",
    "print(gdf_polygons[\"Gi*_Category\"])\n",
    "print('')\n",
    "print(gdf_polygons[\"Gi*_Category\"].value_counts())\n",
    "print('')\n",
    "print(gdf_polygons[\"Gi*\"].isna().sum())\n",
    "print('')\n",
    "print(gdf_polygons[\"Gi*\"].min(), gdf_polygons[\"Gi*\"].max())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeea422",
   "metadata": {},
   "source": [
    "### Step 7: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a color coding for different significance levels\n",
    "def get_color(category):\n",
    "    color_mapping = {\n",
    "        \"Strong Hotspot (p<0.01)\": \"#b10026\",  # Dark Red\n",
    "        \"Moderate Hotspot (p<0.05)\": \"#fd8d3c\",  # Orange-Red\n",
    "        \"Weak Hotspot (p<0.10)\": \"#fecc5c\",  # Light Orange\n",
    "        \"Strong Coldspot (p<0.01)\": \"#084594\",  # Dark Blue\n",
    "        \"Moderate Coldspot (p<0.05)\": \"#3182bd\",  # Medium Blue\n",
    "        \"Weak Coldspot (p<0.10)\": \"#9ecae1\",  # Light Blue\n",
    "        \"Not Significant\": \"gray\"\n",
    "    }\n",
    "    return color_mapping.get(category, \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert polygons to EPSG:4326 (compatible with Folium)\n",
    "gdf_polygons_folium = gdf_polygons.to_crs(epsg=4326)\n",
    "\n",
    "# Create a Folium map centered on Zurich\n",
    "m = folium.Map(location=[47.3769, 8.5417], zoom_start=12, tiles=\"CartoDB positron\")\n",
    "\n",
    "for _, row in gdf_polygons_folium.iterrows():\n",
    "    folium.GeoJson(\n",
    "        row.geometry,\n",
    "        style_function=lambda feature, cat=row[\"Gi*_Category\"]: {\n",
    "            \"fillColor\": get_color(cat),\n",
    "            \"color\": \"black\",\n",
    "            \"weight\": 1,\n",
    "            \"fillOpacity\": 0.6\n",
    "        }\n",
    "    ).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d700aa",
   "metadata": {},
   "source": [
    "## **Final Choice of Methodology**\n",
    "After testing different approaches, we found that the most effective method for detecting significant spatial patterns was to:\n",
    "1. **Use Queen Contiguity Weights (`w_queen`) instead of KNN**  \n",
    "   - Queen contiguity defines neighbors based on shared boundaries, ensuring that spatial relationships reflect actual geographic adjacency rather than arbitrary distance thresholds.\n",
    "   - This method is more appropriate for polygon-based analyses, as it captures natural administrative or statistical boundaries.\n",
    "\n",
    "2. **Avoid Log Normalization of the NUMPOINTS Variable**  \n",
    "   - Log transformation is often used to reduce skewness in highly dispersed data.  \n",
    "   - However, in this case, applying log normalization suppressed variability in the data, reducing the contrast between areas with significantly high and low values.\n",
    "   - By keeping the raw values, we allow the Getis-Ord G\\* statistic to better differentiate between **true hotspots** and **non-significant areas**.\n",
    "\n",
    "### **Why This Works**\n",
    "- The **Queen contiguity approach** better reflects spatial dependence among administrative zones.  \n",
    "- Retaining the **original scale of NUMPOINTS** allows the **Getis-Ord G\\*** analysis to detect real clusters.  \n",
    "- As a result, two statistically significant **hotspots** were successfully identified.  \n",
    "\n",
    "This approach provides a **robust and interpretable spatial analysis**, ensuring that the identified patterns are meaningful within the geographic context of Zurich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d69b80",
   "metadata": {},
   "source": [
    "## **Results & Interpretation**\n",
    "\n",
    "### **Global Autocorrelation (Moran’s I)**\n",
    "The **Global Moran’s I** analysis indicates that the overall spatial distribution of protected historical sites in Zurich does not exhibit strong clustering or dispersion patterns. While some degree of spatial autocorrelation is present, it is not statistically significant at conventional levels (p < 0.05), suggesting that the sites are somewhat evenly distributed across the city rather than forming a single dominant cluster.\n",
    "\n",
    "### **Local Hotspot Analysis (Getis-Ord G\\*)**\n",
    "The **Getis-Ord G\\*** analysis reveals a **weak but statistically significant (p < 0.1) hotspot** in the **Lindenhof neighborhood**, while all other neighborhoods are classified as **not significant**. This suggests that, while there is a concentration of protected historical sites in the city center, it is not overwhelmingly dominant compared to the rest of Zurich.\n",
    "\n",
    "### **Interpretation of the Results**\n",
    "The presence of a weak hotspot in the **Lindenhof neighborhood** aligns with expectations, as many historical and culturally significant buildings are located in Zurich’s central district. However, the absence of strong clustering in other neighborhoods suggests that protected heritage sites are not exclusively concentrated in the city center. \n",
    "\n",
    "This **spatially distributed pattern of heritage protection** can be interpreted as a **positive indicator of urban heritage management policies**. It suggests that authorities are actively protecting historical buildings in peripheral and suburban areas as well, rather than focusing solely on the city center. This **decentralized approach** to heritage conservation contributes to a more **inclusive and widespread recognition of cultural assets** across Zurich.\n",
    "\n",
    "### **Possible Limitations**\n",
    "- The **significance threshold (p < 0.1)** is relatively weak, meaning the detected hotspot in the city center is **not strongly conclusive**.\n",
    "- The **spatial scale** (statistical districts) may influence the results, as smaller-scale clustering patterns could be overlooked.\n",
    "- Alternative spatial weighting methods (e.g., distance-based rather than contiguity-based) may yield different results and should be considered in further research.\n",
    "\n",
    "### **Final Remarks**\n",
    "While Zurich’s **Lindenhof neighborhood** shows some concentration of protected heritage sites, the overall **absence of strong clustering** suggests a well-distributed heritage conservation strategy. Further investigation with different spatial scales and methodologies could provide additional insights into Zurich’s cultural heritage protection efforts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
